---
title: "p8105_hw6_al4225"
author: "Anjing"
date: "2022-11-30"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(dplyr)
library(modelr)
knitr::opts_chunk$set(
  fig.width = 7,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 2
### Raw data
```{r}
homicide_data = read_csv("./data/homicide-data.csv",
                col_names = TRUE) %>%
  janitor::clean_names() 
homicide_data
```

### Clean and tidy
Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.
```{r}
homicide = 
  homicide_data %>%
  mutate(
    city_state = str_c(city, ", ", state)) %>%
  mutate(disposition_situation = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), "unsolved", "solved"),
         #disposition_situation = as.factor(disposition_situation)
         ) %>%

  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"), 
         victim_race %in% c("White", "Black")) %>%
  mutate(victim_age = as.numeric(victim_age))
homicide
```

### Logistic regression for Baltimore, MD
For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 
```{r}
Baltimore = 
homicide %>%
  filter(city_state == "Baltimore, MD") %>%
  mutate(
    disposition_situation = ifelse(disposition_situation == "solved", 1, 0),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(disposition_situation, victim_age, victim_race, victim_sex)

glm = glm(Baltimore$disposition_situation ~ Baltimore$victim_age + Baltimore$victim_race + Baltimore$victim_sex, 
          family = binomial(link = "logit"))

summary(glm)

```

### Odds ratio
Apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.   

**Discription**: For male and female victims, OR = 0.426 < 1 and is included in the confidence interval. Sex and solving homicides are negative related which means men(1) victims tend to have unsolved homicides. The OR of men victims whose homicides were solved is 0.426 times than women victims. So homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female.
```{r}
glm %>%
  broom::tidy()

glm %>%
  broom::tidy() %>%
  mutate(OR = exp(estimate),
         conf.low = exp(estimate - 1.96 * std.error),
         conf.high = exp(estimate + 1.96 * std.error)) %>%
  select(term, log_OR = estimate, OR, p.value, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)

#confidence interval of OR--all include estimated value
exp(confint(glm))
```

### Save result as .RData
Save the output of glm as an R object;
```{r}
save(glm, file = "test/glm_baltimore_result.RData")
```

### glm for each city
Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.
```{r}
all_cities = 
  homicide %>%
  group_by(city_state) %>% 
  mutate(
    disposition_situation = ifelse(disposition_situation == "solved", 1, 0),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(disposition_situation, victim_age, victim_race, victim_sex)
all_cities
```

```{r}
results_cities = 
  all_cities %>% 
  nest(data = -city_state) %>%
  mutate(
    glm_cities = map(.x = data,
                      ~glm(disposition_situation ~ victim_age + victim_race + victim_sex, data = .x, family = binomial(link = "logit"))),
    tidy_tests = map(.x = glm_cities, ~broom::tidy(.x))
  ) %>%
  select(city_state, tidy_tests) %>% 
  unnest(tidy_tests) %>% 
  mutate(OR = exp(estimate),
         conf.low = exp(estimate - 1.96 * std.error),
         conf.high = exp(estimate + 1.96 * std.error)) %>%
  select(term, log_OR = estimate, OR, p.value,  conf.low, conf.high) 
results_cities
```

### Plot
Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.    
**Descriptions**: According to the plot, Albuquerque, NM has the OR_max and New York, NY has OR_min. That means the OR(>1) of solving homicides comparing male victims to female victims is higher in Albuquerque, NM. So homicides in which the victim is male are significantly more likely to be resolved than those in which the victim is female in Albuquerque, NM.
```{r results_cities_plot}
results_cities_plot = 
results_cities %>% 
  filter(term == "victim_sexMale") %>%
  group_by(city_state) %>%
  arrange() %>%
  ungroup() %>%
  mutate(city_state = fct_reorder(city_state, OR, .desc = TRUE)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  geom_smooth(se = TRUE) +
  labs(
    title = "Solving homicides comparing male victims to female victims",
    x = "City_state",
    y = "OR",
    caption = "Data from Washington Post "
  ) +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),
        plot.title = element_text(hjust = 0.5))
results_cities_plot
```


## Problem3
### Raw data
Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).
```{r}
birthweight_data = read_csv("./data/birthweight.csv",
                col_names = TRUE) %>%
  janitor::clean_names() %>%
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
birthweight_data
nrow(birthweight_data)

birthweight_data %>%
  drop_na() %>%
  nrow()
```
**Discriptions**: There are `r nrow(birthweight_data)` observations and `r ncol(birthweight_data)` variables in this cleaned dataset. There is no missing data at all. 

### Regression
Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process.     


According to researches, we have the following information about underly birthweight.        
1.Age: Mother at the age of 15 or younger or 35 or older brings complications and birth defects.      
2.Weight: Mothers' low weight and insufficient nutrition before delivering can contribute to the low weight of newborns.     
3.Race: It is proven that black women are more likely to give birth to a child who has a low birth weight.    
4.Smoke: Babies of mothers who are exposed to cigarettes are more likely to have low birthweight.     
5.fincome: Mothers of lower socioeconomic status are also more likely to have poorer pregnancy nutrition, inadequate prenatal care, and pregnancy complications — all factors that can contribute to low birthweight.     


So I will choose momage(mother’s age at delivery (years)), delwt(mother’s weight at delivery (pounds)),  smoken(average number of cigarettes smoked per day during pregnancy), fincome(family monthly income (in hundreds, rounded)), mrace: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other) as predictors. In this model, we just consider the main effects.

```{r}
bwt = lm(bwt ~ momage + delwt + mrace + smoken + fincome, data = birthweight_data)
summary(bwt)
```

### Model table
```{r}
bwt %>%
  broom::tidy()
```

### Plots
Show a plot of model residuals against fitted values – use `add_predictions` and `add_residuals` in making this plot.    
**Description**: The residual-fitted values plot reflects the normality of the dots. The dots are randomly scattered around the line y=0, the dots seem like symmetric. So we can assume the linear regression model has residuals with the normal distribution.
```{r residual_plot}
residual_plot = 
birthweight_data %>% 
  modelr::add_residuals(bwt) %>%
  modelr::add_predictions(bwt) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(se = F, color = "red", method = "lm") +
  theme_bw() +
  labs(
    title = "Model residuals against fitted values",
    x = "Pred",
    y = "Pesid",
    caption = "Data consists of roughly 4000 children "
  ) +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(hjust = 1, vjust = .5),
        plot.title = element_text(hjust = 0.5))
residual_plot
```

### Two other models
One using length at birth and gestational age as predictors (main effects only).     
One using head circumference, length, sex, and all interactions (including the three-way interaction) between these.
```{r}
model1 = lm(bwt ~ blength + gaweeks, data = birthweight_data)
model2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_data)

summary(model1)
summary(model2)
```

### Comparison
Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.    
The RMSEs are suggestive that the two other models work better than the first model bwt, and that the model2 is better than the model1.
```{r}
#Compute root mean squared errors (RMSEs) for each model.
rmse(bwt, birthweight_data)
rmse(model1, birthweight_data)
rmse(model2, birthweight_data)

#CV
cv_df = 
  crossv_mc(birthweight_data, 100) %>%
  mutate(
    bwt  = map(train, ~lm(bwt ~ momage + delwt + mrace + smoken + fincome, data = .x)),
    model1  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model2  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, , data = .x))) %>% 
  mutate(
    rmse_bwt = map2_dbl(bwt, test, ~rmse(model = .x, data = .y)),
    rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)))

```

### Plot
**Description**:    
The plot below shows the distribution of RMSE values for each candidate model. now we get a sense of variance in prediction error and can compare prediction error distributions across methods. According to the plot, model bwt has the highest RMSE, model2(one using head circumference, length, sex, and all interactions (including the three-way interaction) between these) has the lowest RMSE. So the best model is model2.
```{r RMSE_plot}
RMSE_plot = 
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin(aes(fill = model)) +
  labs(
    title = "RMSE in each model",
    caption = "Data consists of roughly 4000 children"
  ) +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(hjust = 1, vjust = .5),
        plot.title = element_text(hjust = 0.5))
RMSE_plot
```
